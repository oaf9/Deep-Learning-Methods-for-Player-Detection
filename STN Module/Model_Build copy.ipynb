{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import zipfile\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from sp_module import Net\n",
    "from train import Network_Trainer\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from DataSet import JerseyDataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from SoccerNet.Downloader import SoccerNetDownloader as SNdl\n",
    "# mySNdl = SNdl(LocalDirectory=\"SoccerNet\")\n",
    "# mySNdl.downloadDataTask(task=\"jersey-2023\", split=[\"train\",\"test\",\"challenge\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function will load the data and return two zipped arrays: \n",
    "\n",
    "output:       contains an image stored as a numpy array\n",
    "class_labels: contains the corresponding label for each element of output\n",
    "\n",
    "@params\n",
    "dir: should be the path that contains both train images, and json dic with labels\n",
    "save: If you want it to save the output as a zip file (so you don't have to do this twice)\n",
    "\"\"\"\n",
    "\n",
    "def load_data(dir, save = False):\n",
    "    #labels is a dictionary mapping file_numer ->class_label\n",
    "    with open(dir + \"/train_gt.json\") as file:\n",
    "        labels = json.load(file)\n",
    "\n",
    "    #converts an image to a numpy array\n",
    "        \n",
    "    \"\"\"\n",
    "    Eventually we will want to resize the images when we localize. This is just so that they are all the same size.\n",
    "    \"\"\"\n",
    "    get_image = lambda file_name: cv2.resize(cv2.imread(file_name),(40, 100))\n",
    "\n",
    "    output = []\n",
    "    class_labels = []\n",
    "\n",
    "    #iterate through the folders, convert the images to RGB arrays, and then append the class label\n",
    "    for folder in list(os.listdir(dir+\"/images\")):\n",
    "\n",
    "        if folder == '.DS_Store': continue\n",
    "        #if (folder != '1' and folder != '2'): continue\n",
    "\n",
    "        cls = labels[folder]\n",
    "        images = os.listdir(os.path.join(dir+\"/images\", folder))\n",
    "\n",
    "        for image in images:\n",
    "            output.append(get_image(os.path.join(dir+\"/images\", folder, image)))\n",
    "            class_labels.append(cls)\n",
    "\n",
    "    zip_file = zip(output, class_labels)\n",
    "    if save:\n",
    "        np.savez_compressed(dir+\"/numpy_data.npz\", output, labels)\n",
    "        \n",
    "    return zip_file\n",
    "\n",
    "\n",
    "def load_data_from_list(dir_train, dir_test, train_list = None, test_list = None):\n",
    "        \n",
    "\n",
    "    train_images = []\n",
    "    train_targets=[]\n",
    "\n",
    "    test_images = []\n",
    "    test_targets = []\n",
    "\n",
    "    get_image = lambda file_name: cv2.resize(cv2.imread(file_name),(40, 100))\n",
    "\n",
    "    #get train labels\n",
    "    with open('/Users/omarafifi/Downloads/Jersey Detection/Notebooks And Modules/SoccerNet/jersey-2023/train/train_gt.json') as train_file:\n",
    "        train_labels = json.load(train_file)\n",
    "\n",
    "    #get test labels\n",
    "    with open('/Users/omarafifi/Downloads/Jersey Detection/Notebooks And Modules/SoccerNet/jersey-2023/test/test_gt.json') as test_file:\n",
    "        test_labels = json.load(test_file)\n",
    "    \n",
    "    \n",
    "    # cls = labels[folder]\n",
    "\n",
    "\n",
    "    #get all the training images\n",
    "    for index, train_image_name in enumerate(train_list):\n",
    "\n",
    "\n",
    "        file_number = train_image_name.split('_')[0]\n",
    "\n",
    "        image_path = dir_train + '/images/'+ file_number + '/'+ train_image_name\n",
    "\n",
    "        class_label = train_labels[file_number]\n",
    "        train_targets.append(class_label)\n",
    "\n",
    "        train_images.append(get_image(image_path))\n",
    "\n",
    "    for index, test_image_name in enumerate(test_list):\n",
    "\n",
    "        file_number = test_image_name.split('_')[0]\n",
    "\n",
    "        image_path = dir_test + '/images/'+ file_number + '/'+ test_image_name\n",
    "\n",
    "        class_label = test_labels[file_number]\n",
    "        test_targets.append(class_label)\n",
    "\n",
    "        test_images.append(get_image(image_path))\n",
    "\n",
    "    return train_images, train_targets, test_images, test_targets\n",
    "\n",
    "#load the google data\n",
    "def load_google_data(dir):\n",
    "\n",
    "    dir = '/Users/omarafifi/Downloads/Jersey Detection/Data/Google Images'\n",
    "\n",
    "    with open(dir + \"/google_gt.json\") as file:\n",
    "        google_labels = json.load(file)\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    dir = '/Users/omarafifi/Downloads/Jersey Detection/Data/Google Images/google_jerseys'\n",
    "\n",
    "    for folder in list(os.listdir(dir)):\n",
    "            \n",
    "            get_image = lambda file_name: cv2.resize(cv2.imread(file_name),(32,32))\n",
    "\n",
    "            if folder == '.DS_Store': continue\n",
    "\n",
    "\n",
    "            #if (folder != '1' and folder != '2'): continue\n",
    "            images = os.listdir(os.path.join(dir, folder))\n",
    "            #print(images)\n",
    "\n",
    "            for image in images:\n",
    "\n",
    "                if image == None: continue\n",
    "                if image == '.DS_Store': continue\n",
    "\n",
    "                X.append(get_image(os.path.join(dir, folder, image)))\n",
    "                y.append(google_labels[folder+'_'+image])\n",
    "\n",
    "    return X, y\n",
    "\n",
    "#introduce some of the google images into the training and test data to ensure the same number of labels\n",
    "def stratify_google(X_trn, X_tst, X_goog, y_trn, y_tst, y_goog ):\n",
    "\n",
    "    X_goog_tr, X_goog_tst, y_goog_tr, y_goog_tst = train_test_split(X_goog, y_goog, stratify= y_goog, train_size=.9)\n",
    "    X_train = np.concatenate((X_trn, X_goog_tr), axis = 0)\n",
    "    X_test = np.concatenate((X_tst, X_goog_tst), axis = 0)\n",
    "    y_train = np.concatenate((y_trn, y_goog_tr), axis = 0)\n",
    "    y_test = np.concatenate((y_tst, y_goog_tst), axis = 0)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "def load_small_data(dir):\n",
    "\n",
    "    data = np.load(dir)\n",
    "    lst = data.files\n",
    "    X = data[lst[0]]\n",
    "    y = data[lst[1]]\n",
    "\n",
    "    return X,y\n",
    "\n",
    "def resize(width, height, images):\n",
    "    output = []\n",
    "\n",
    "    for image in images: \n",
    "        output.append(cv2.resize(image, (width, height)))\n",
    "    return output\n",
    "\n",
    "\n",
    "def crop_images(data):\n",
    "\n",
    "    Images = data.copy()\n",
    "    \n",
    "    def crop_bottom(image, percent):\n",
    "        height, width = image.shape[:2]\n",
    "        cutoff = int(height * (1 - percent))\n",
    "        cropped_image = image[:cutoff, :]\n",
    "        return cropped_image\n",
    "\n",
    "    def crop_top(image, percent):\n",
    "        height, width = image.shape[:2]\n",
    "        cutoff = int(height * percent)\n",
    "        cropped_image = image[cutoff:, :]\n",
    "        return cropped_image\n",
    "\n",
    "    def crop_lr(image, percent):\n",
    "        height, width = image.shape[:2]\n",
    "        crop_width = int(width * percent)\n",
    "        cropped_image = image[:, crop_width:-crop_width]\n",
    "        return cropped_image\n",
    "\n",
    "\n",
    "    for index, img in enumerate(Images):\n",
    "        \n",
    "        img= crop_bottom(img, 0.5)\n",
    "        img = crop_top(img, 0.3)\n",
    "        img = crop_lr(img, 0.2)\n",
    "        img = cv2.resize(img, (32,32))\n",
    "        Images[index] = img\n",
    "\n",
    "\n",
    "    return Images\n",
    "\n",
    "def augment(X):\n",
    "    out = []\n",
    "    for img in X:\n",
    "        scale = 3.0\n",
    "        height, width = img.shape[:2]\n",
    "        # Upscale\n",
    "        scale = 3.0\n",
    "        img = cv2.resize(img, (round(scale * width), round(scale * height)), interpolation=cv2.INTER_CUBIC)\n",
    "        img = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "        out.append(img)\n",
    "\n",
    "    return np.array(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir_train = '/Users/omarafifi/Downloads/Jersey Detection/Data/SoccerNet/jersey-2023/train'\n",
    "# dir_test='/Users/omarafifi/Downloads/Jersey Detection/Data/SoccerNet/jersey-2023/test'\n",
    "# train_list = np.genfromtxt('/Users/omarafifi/Downloads/Jersey Detection/Data/Good Images/train_img_with_numbers.txt', dtype='str')\n",
    "# test_list = np.genfromtxt('/Users/omarafifi/Downloads/Jersey Detection/Data/Good Images/test_img_with_numbers.txt', dtype = 'str')\n",
    "dir_train = '/Users/omarafifi/Downloads/Jersey Detection/Notebooks And Modules/SoccerNet/jersey-2023/train'\n",
    "dir_test='/Users/omarafifi/Downloads/Jersey Detection/Notebooks And Modules/SoccerNet/jersey-2023/test'\n",
    "train_list = np.genfromtxt('/Users/omarafifi/Downloads/Jersey Detection/Data/Paddle Output/paddle_train.csv', dtype='str')\n",
    "test_list = np.genfromtxt('/Users/omarafifi/Downloads/Jersey Detection/Data/Paddle Output/paddle_test.csv', dtype = 'str')\n",
    "\n",
    "X_trn, y_train, X_tst, y_test = load_data_from_list(dir_train=dir_train,\n",
    "                    dir_test= dir_test, train_list=train_list, test_list=test_list)\n",
    "\n",
    "with open(\"/Users/omarafifi/Downloads/Jersey Detection/Notebooks And Modules/SoccerNet/jersey-2023/train/train_gt.json\") as file:\n",
    "    train_labels = json.load(file)\n",
    "with open(\"/Users/omarafifi/Downloads/Jersey Detection/Notebooks And Modules/SoccerNet/jersey-2023/test/test_gt.json\") as file:\n",
    "    test_labels = json.load(file)\n",
    "\n",
    "X_google, y_google = load_google_data('/Users/omarafifi/Downloads/Jersey Detection/Data/Google Images/google_jerseys')\n",
    "X_train, X_test = crop_images(X_trn), crop_images(X_tst)\n",
    "X_train, X_test, y_train, y_test = stratify_google(X_train, X_test, X_google, y_train, y_test, y_google )\n",
    "X_train, X_test = augment(X_train), augment(X_test)\n",
    "\n",
    "#Recover the label mapping\n",
    "label_to_int = {}\n",
    "int_to_label = {}\n",
    "\n",
    "for index, label in enumerate(np.unique(y_train)):\n",
    "    label_to_int[label]=index\n",
    "    int_to_label[index]=label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# right now, there is no data processing. \n",
    "# But, We need to make sure that all the images have the same shape.\n",
    "# and one hot encode y labels\n",
    "def process_data(X_data, y_data):\n",
    "\n",
    "    \"\"\"\n",
    "    Prepare the response:\n",
    "\n",
    "    This part of the function just basically maps the classes to the set {0, ... 44}, and then one-hot encodes the response\n",
    "    \"\"\"\n",
    "    #for to_categorical to work, we need to map the labels to {0, ... 57}\n",
    "    label_to_int = {}\n",
    "    int_to_label = {}\n",
    "\n",
    "    for index, label in enumerate(np.unique(y_data)):\n",
    "        label_to_int[label]=index\n",
    "        int_to_label[index]=label\n",
    "\n",
    "    \n",
    "    y_data_new = np.vectorize(label_to_int.__getitem__)(y_data)\n",
    "    y_data_new = to_categorical(y_data_new)\n",
    "\n",
    "    #X_train, y_train = zip(*load_data(directory, True))\n",
    "    X_data = np.array(X_data).reshape(-1, 3, 96, 96)\n",
    "\n",
    "\n",
    "    return X_data, y_data_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = process_data(X_train, y_train)\n",
    "X_test, y_test = process_data(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Spacial Transformer Network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing Hyperparamaters for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = X_train.mean(axis=(0,2,3))/255\n",
    "devs = X_train.std(axis=(0,2,3))/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 1\n",
    "batch_size =  24\n",
    "\n",
    "#need to compute the means and standard deviations to batch normalize: \n",
    "\n",
    "\n",
    "sp_transform = transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    v2.RandomHorizontalFlip(p=0.5),\n",
    "                    transforms.Normalize(means,devs)\n",
    "                    ])\n",
    "\n",
    "device =  torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepping the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spt = Net()\n",
    "optimizer = optim.SGD(spt.parameters(), lr = learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_tensor_x = torch.Tensor(X_train) # transform to torch tensor\n",
    "train_tensor_y = torch.Tensor(y_train)\n",
    "\n",
    "val_tensor_x = torch.Tensor(X_test) # transform to torch tensor\n",
    "val_tensor_y = torch.Tensor(y_test)\n",
    "\n",
    "train_dataset = TensorDataset(train_tensor_x, train_tensor_y)\n",
    "val_dataset = TensorDataset(val_tensor_x, val_tensor_y)\n",
    "\n",
    "trainer = Network_Trainer(model = spt, \n",
    "                          transforms = sp_transform, criterion = criterion, \n",
    "                          epochs = epochs, device = device, optimizer = optimizer,\n",
    "                          batch_size = batch_size, \n",
    "                          learning_rate = learning_rate, out_path = '/Users/omarafifi/Downloads/Jersey Detection/Data/Output Images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = JerseyDataset(X_train, y_train[:100], sp_transform)\n",
    "val_data = JerseyDataset(X_test[:100], y_test[:100], sp_transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size = batch_size, shuffle = True )\n",
    "val_loader = DataLoader(val_data, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 1\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:01,  4.72it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 18.70it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2069, Train Acc: 0.00\n",
      "Validation Loss: 0.2018, Val Acc: 0.00\n"
     ]
    }
   ],
   "source": [
    "trainer.train_model(train_loader, train_data, val_loader, val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Challenge Data and Obtain The Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_list = np.genfromtxt('/Users/omarafifi/Downloads/Jersey Detection/Data/Paddle Output/paddle_challenge.csv', dtype = 'str')\n",
    "pth = '/Users/omarafifi/Downloads/Jersey Detection/Notebooks And Modules/SoccerNet/jersey-2023/challenge/images'\n",
    "\n",
    "def get_challenge_data(challenge_list, path):\n",
    "\n",
    "    get_image = lambda file_name: cv2.resize(cv2.imread(file_name),(40, 100))\n",
    "    folders = []\n",
    "\n",
    "    X_challenge =[]\n",
    "\n",
    "    print('Getting Folder Names')\n",
    "    for i in range(1426):\n",
    "        i = str(i)\n",
    "        out = np.flatnonzero(np.char.startswith(challenge_list, i + '_'))\n",
    "        folders.append(challenge_list[out])\n",
    "\n",
    "\n",
    "\n",
    "    print('Getting Images')\n",
    "    for index, folder in enumerate(folders):\n",
    "        #contains a list of images\n",
    "        point = []\n",
    "        for image in folder:\n",
    "\n",
    "            image_path = path + '/' + str(index) + '/' + image\n",
    "            point.append(get_image(image_path))\n",
    "\n",
    "        point = crop_images(point)\n",
    "        point = augment(point)\n",
    "\n",
    "        X_challenge.append(point.reshape(-1, 3, 96, 96))\n",
    "\n",
    "    return  X_challenge\n",
    "\n",
    "\n",
    "def get_predictions(X, trainer):\n",
    "    predictions = []\n",
    "    print('Getting Predictions')\n",
    "    for image_sequence in X:\n",
    "        softmax, preds = trainer.predict(torch.tensor(image_sequence.astype(np.float32)))\n",
    "        most_common = np.bincount(preds).argmax()\n",
    "        predictions.append(most_common)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Folder Names\n",
      "Getting Images\n",
      "Getting Predictions\n"
     ]
    }
   ],
   "source": [
    "X_challenge = get_challenge_data(challenge_list, pth)\n",
    "y = get_predictions(X_challenge, trainer)\n",
    "challenge_predictions = np.vectorize(int_to_label.__getitem__)(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['78', '78', '78', ..., '78', '78', '78'], dtype='<U2')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "challenge_predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jd_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
